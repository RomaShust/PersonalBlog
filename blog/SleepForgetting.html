<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sleep protects memories from catastrophic forgetting</title>
    <meta name="author" content="Roma Shusterman">
    <meta name="keywords" content="olfactory systems, odor delivery, odor intensity" />
    <link href="../style/paper.css" type="text/css" rel="stylesheet" />
	<link href="../style/index.css" rel="stylesheet" />
	<link href="../style/blog.css" rel="stylesheet" />
	<base target="_blank">
  </head>
  
  
  <body>
	  <div class="wrapper">  
			<div class = "grid">
				<div class = "header">
					<div> <a class = "header_ref" href="../index.html" target="_self"> Roma Shusterman </a> </div>
					<div> <a class = "header_ref" href="../research.html" target="_self"> Research </a> </div>
					<div> <a class = "header_ref" href="https://www.linkedin.com/in/roma-shusterman/"> LinkedIn </a>  </div>  
				</div>

				<div class = "title">Sleep protects memories from catastrophic forgetting</div>
				
				<div class = "blog_content"> The initial inspiration for artificial neural networks (AANs) comes from the structure and 
											connectivity of the biological neural networks. The original implementation of the AANs 
											loosely models the biological neurons. Two different research approaches deviate from that 
											point. The first approach is to build more efficient neural networks that will solve practical
											 problems. Recurrent neural networks demonstrated a significant step forward in ANN performance, 
											 however, methods used to train these recurrent models are not biologically plausible. The second 
											 approach is biologically inspired. Examples from this approach are spiking neural networks 
											 (biologically realistic models of neurons to carry out computation), long-short term memory 
											 (LSTM), and attention building blocks. 
				</div>	
				
				<div class = "blog_content"> A theoretical group from UCSD lead by Prof. Maxim Bazhenov went further mimicking not only the 
											biological blocks but also biological processes. One of the purposes of our sleep is to consolidate 
											daily acquired memories. They asked if by mimicking the sleep process in the neural network, the 
											ANNs can be trained to perform multiple tasks and not suffer from catastrophic forgetting. Catastrophic 
											forgetting is an inability of the network to learn knowledge without forgetting the previous one. 
											It happens because new training likely overrides the old synaptic weights (Figure 1). 
				</div>

				<img class = "blog_image" src = "../images/CatastrophicForgetting.png">

				<div class = "image_legend"> Figure 1: Concept of catastrophic forgetting. Source: Attention-Based Selective Plasticity 
														(Kolouri et al., 2019)
				</div>
				
				<div class = "blog_content"> The previous approach to allow the neural network to remembers old tasks was achieved by selectively 
											slowing down learning on the most contributing synaptic weights for those tasks by introducing punishment 
											via quadratic penalties to the loss function [1]. Bazhenov's group approach was motivated by the 
											importance of sleep in learning and memory. They hypothesized that simulating sleep after learning will 
											reverse the forgetting and enhance both old and new memories. Inspired by the processes involved in 
											sleep generation in biological networks, the group developed an algorithm that implements a sleep-like 
											phase in neural networks. They demonstrated that "sleep"â€‹ can recover older tasks that were otherwise 
											forgotten [2]. 
				</div>

				<div class = "blog_content"> Gonzalez and colleagues developed a network model of memory encoding in a thalamocortical network. 
					They compared two different scenarios:<br><br>
					1. Training on Task A -> Training on Task B<br><br>

					2. Training on Task A -> Sleeping ->Training on Task B<br>
				</div>

				<div class = "blog_content"> They found that implementation of a sleep-like phase in ANNs reduces catastrophic forgetting and 
											transfer of knowledge. They demonstrated that sleep creates unique representations of each class of 
											inputs and that relevant neurons fire during sleep, simulating the replay of previously learned 
											memories. Interestingly, while the previous study [1] forced the synaptic weights to stay the same 
											by introducing punishment, Gonzalez and colleagues observed that sleep replay changed the synaptic 
											footprint of the old memory to allow overlapping neuronal populations to store multiple memories.
				</div>

				<div class = "blog_content"> To summarize, we compared neural activity in the mouse olfactory bulb with odor 
					intensity perception in humans. We found the two to be consistent.  Odor responses changed with decreasing 
					concentration and repeated sampling of a constant odor source in similar ways. Using a classifier, we found 
					that the odor concentration coded on later sniffs was sharply lower than on the first sniff. These neural 
					results were consistent with the sharp sniff to sniff drop in odor intensity reported by human volunteers. 
					Our data suggest that responses of neurons in the olfactory bulb are consistent with odor intensity perception.
				</div>
				
				<div class = "manuscript">
					<ol>
						<li> Kirkpatrick J et al.
							<a href="https://www.pnas.org/content/114/13/3521">
							Overcoming catastrophic forgetting in neural networks.</a> 2017 PNAS 114 (13).</li>
						<li> Gonzalez OC et al.
							<a href="https://elifesciences.org/articles/51005">
							Can sleep protect memories from catastrophic forgetting?</a> 2020 ELife 9:e51005.</li>

					</ol>
				</div>
				
				<div class = "footer">  </div>

			</div>
		</div> 
	</body>
</html>
